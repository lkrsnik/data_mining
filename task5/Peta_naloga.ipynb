{
 "metadata": {
  "name": "",
  "signature": "sha256:7568df864b8a1f3d4f616822db2adafdcca3b300d5b7205814e7d1bdc5e35a72"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.set_printoptions(suppress=True, precision=16)\n",
      "\n",
      "import Orange\n",
      "from Orange.classification import Model, Learner\n",
      "from Orange.data import Table, Domain, DiscreteVariable, ContinuousVariable\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_COLS = 94\n",
      "NUM_OF_CLASSSES = 9\n",
      "E = 1e-8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def g(theta, x):\n",
      "    return 1. / (1 + np.exp(-x.dot(theta)))\n",
      "\n",
      "def softmax(a, theta):\n",
      "        z = a.dot(theta)\n",
      "        ps = np.exp(z - np.max(z, axis=1)[:,None])\n",
      "        ps /= np.sum(ps, axis=1)[:, None]\n",
      "        return ps\n",
      "\n",
      "def add_ones(X):\n",
      "    return np.column_stack((np.ones(len(X)), X))\n",
      "\n",
      "def read_data(fname, read_class=True):\n",
      "    X = np.loadtxt(fname, delimiter=',', skiprows=1, usecols=range(1,NUM_OF_COLS))\n",
      "    atts = [ContinuousVariable(\"feature_\" + str(i)) for i in range(1, NUM_OF_COLS)]\n",
      "    class_var = DiscreteVariable(\"class\", values=[i for i in range(NUM_OF_CLASSSES)])\n",
      "    domain = Domain(atts, class_var)\n",
      "    if read_class:\n",
      "        y = np.loadtxt(fname, delimiter=',', skiprows=1, usecols=range(NUM_OF_COLS,NUM_OF_COLS+1), dtype=np.dtype(str))\n",
      "        y = np.array([float(c[-2]) - 1 for c in y])\n",
      "    else:\n",
      "        y = np.zeros(len(X))\n",
      "    return Table(domain, X, y)\n",
      "\n",
      "def numerical_grad(f, params, epsilon):\n",
      "    num_grad = np.zeros(len(params))\n",
      "    perturb = np.zeros(len(params))\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad\n",
      "\n",
      "def indicator(Y, k):\n",
      "    ind = np.zeros(Y.shape)\n",
      "    for i in range(k):\n",
      "        ind_i = Y == i\n",
      "        ind = np.column_stack((ind, ind_i.astype(int)))\n",
      "    ind = ind[:,1:]\n",
      "    return ind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogLoss(Orange.evaluation.scoring.Score):\n",
      "    def log_loss(self, y, P):\n",
      "        m = P.shape[0]\n",
      "        \n",
      "        Y = np.zeros(P.shape)\n",
      "        for i in range(Y.shape[1]):\n",
      "            Y[y == i,i] = 1\n",
      "            \n",
      "        almost_ones = np.ones(P.shape) - E\n",
      "        almost_zeros = np.zeros(P.shape) + E\n",
      "        \n",
      "        log = np.log(np.maximum(np.minimum(P, almost_ones), almost_zeros))\n",
      "        \n",
      "        return (-1/m) * np.sum(Y * log)\n",
      "        \n",
      "    def compute_score(self, results):\n",
      "        domain = results.domain\n",
      "        n_classes = len(domain.class_var.values)\n",
      "        \n",
      "        scores = []\n",
      "        \n",
      "        for ps in results.probabilities:\n",
      "            scores.append(metrics.log_loss(results.actual, ps))\n",
      "            \n",
      "        return np.array(scores)\n",
      "\n",
      "class ParameterFit:\n",
      "    def __init__(self, learner, param_set):\n",
      "        self.learner = learner\n",
      "        self.param_set = param_set\n",
      "    def __call__(self, data, return_learner=False):\n",
      "        learners = [self.learner(p) for p in self.param_set]\n",
      "        res = Orange.evaluation.CrossValidation(data, learners, k=10)\n",
      "        scores = LogLoss(res).tolist()\n",
      "        #print(self.param_set)\n",
      "        #print(scores)\n",
      "        min_i = scores.index(np.min(scores))\n",
      "        best_p = self.param_set[min_i]\n",
      "        best_learner = self.learner(best_p)\n",
      "        if return_learner:\n",
      "            return best_learner\n",
      "        return best_learner(data)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier(Orange.classification.Model):\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, domain, thetas):\n",
      "        super().__init__(domain)\n",
      "        self.thetas = thetas  # model parameters\n",
      "\n",
      "    def predict(self, X):\n",
      "        return self.h(X, self.thetas)\n",
      "        #y_hat = np.ravel(self.h(X, self.thetas))\n",
      "        # following works only for binary classifiers, correct it for multiclass\n",
      "        #return np.vstack((1-y_hat, y_hat)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner(Orange.classification.Learner):\n",
      "    def __init__(self, arch, lambda_=1e-5):\n",
      "        super().__init__()\n",
      "        self.arch = arch\n",
      "        self.lambda_ = lambda_\n",
      "        self.name = \"ann\"\n",
      "        self.count = 0\n",
      "\n",
      "        self.theta_shape = np.array([(arch[i]+1, arch[i+1])\n",
      "                                     for i in range(len(arch)-1)])\n",
      "        ind = np.array([s1*s2 for s1, s2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum(ind[:-1])\n",
      "        self.theta_len = sum(ind)\n",
      "\n",
      "    def shape_thetas(self, thetas):\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, shape in enumerate(self.theta_shape)]\n",
      "\n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        #np.random.seed(10)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "        #return np.array([-3, 1, 2, -2, 2, -2, 1, -2, -2])\n",
      "    \n",
      "    def test_grad(self, thetas):\n",
      "        ag = self.costgrad(thetas)[1]\n",
      "        ng = self.grad_approx(thetas, 1e-4)\n",
      "        #print(\"ag\")\n",
      "        #print(ag)\n",
      "        #print(\"ng\")\n",
      "        #print(ng)\n",
      "        print(\"grad diff\")\n",
      "        print(np.sum((ag - ng)**2))\n",
      "        print(\"---------------------------------\")\n",
      "        return np.sum((ag - ng)**2) < E\n",
      "\n",
      "    def grad_approx(self, thetas, e=1e-4):\n",
      "        return np.array([(self.costgrad(thetas+eps, return_cost=True) - self.costgrad(thetas-eps, return_cost=True))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "    def shape_y(self, y):\n",
      "        values = np.unique(y)\n",
      "        y1 = np.zeros((len(y), len(values)))\n",
      "        for v in values:\n",
      "            y1[y == v, v] = 1\n",
      "        return y1\n",
      "\n",
      "    def costgrad(self, thetas_flat, return_cost=False):\n",
      "        # use matrix and vector operations. could be written in a single line\n",
      "        # use self.m as stored by the fit function\n",
      "        y = self.shaped_y\n",
      "        X = self.X\n",
      "        m = self.m\n",
      "        \n",
      "        ################################ COST ################################\n",
      "\n",
      "        term1 = self.h(X, thetas_flat) - y\n",
      "       \n",
      "        term1 = term1 ** 2\n",
      "        term1 = sum(sum(term1))\n",
      "        term1 /= (2*m)\n",
      "        \n",
      "        # calculate regularisation term\n",
      "        thetas = self.shape_thetas(thetas_flat)        \n",
      "        term2 = 0\n",
      "        \n",
      "        for theta in thetas:\n",
      "            theta = theta[1:]\n",
      "            term2 += sum(sum(theta ** 2))\n",
      "        term2 *= (self.lambda_/2)\n",
      "        \n",
      "        cost = term1 + term2\n",
      "        if return_cost:\n",
      "            return cost\n",
      "        \n",
      "        ################################ GRAD ################################\n",
      "       \n",
      "        # 1. Perform a feedforward pass, computing the activations\n",
      "        #self.h(X, thetas_flat) # DONE in cost part\n",
      "        \n",
      "        # 2. For the output layer (layer nl), set error\n",
      "        a = self.activations[-1]\n",
      "        fz = a * (1 - a)\n",
      "        dl1 = (a - y)\n",
      "        deltas = [(a - y)]\n",
      "        \n",
      "        # 3. For l = nl \u2212 1, nl \u2212 2, nl \u2212 3, . . . , 2 set errors and calculate gradient\n",
      "        thetas = self.shape_thetas(thetas_flat)\n",
      "        grads_flat = []        \n",
      "        # activations without last one and thetas must be of same shapes\n",
      "        for a, theta_bias in zip(reversed(self.activations[:-1]), reversed(thetas)):\n",
      "            theta = theta_bias[1:] # remove bias\n",
      "            fz = a * (1 - a)            \n",
      "            dl1 = deltas[0]\n",
      "            \n",
      "            deltas = [dl1.dot(theta.T) * fz] + deltas\n",
      "            \n",
      "            # 4. Compute the desired partial derivatives for all thetas\n",
      "            # add ones to activations to propagate bias to lower levels!!!!\n",
      "            grad_theta = add_ones(a).T.dot(dl1)\n",
      "            grads_flat = grad_theta.ravel().tolist() + grads_flat\n",
      "        grads_flat = np.array(grads_flat)\n",
      "        \n",
      "        # calculate regularisation term\n",
      "        term2 = 0        \n",
      "        for theta in thetas:\n",
      "            theta = theta[1:]\n",
      "            term2 += sum(sum(theta))\n",
      "        term2 *= (self.lambda_)\n",
      "        \n",
      "        grad = (1/m) * grads_flat + term2\n",
      "        \n",
      "        #print (cost)\n",
      "        #print (grad)\n",
      "        \n",
      "        return cost, grad\n",
      "\n",
      "    def h(self, a, thetas):\n",
      "        \"\"\"feed forward, prediction\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        self.activations = [a]\n",
      "        for theta in thetas[:-1]:\n",
      "            a = g(theta, add_ones(a))\n",
      "            self.activations.append(a)\n",
      "            \n",
      "        # softmax last layer\n",
      "        s = softmax(add_ones(a), thetas[-1])\n",
      "        self.activations.append(s)\n",
      "        return s       \n",
      "\n",
      "    def fit(self, X, y, W=None):\n",
      "        self.X, self.y = X, y\n",
      "        #print (y)\n",
      "        self.shaped_y = self.shape_y(self.y)\n",
      "        #print (self.shaped_y)\n",
      "        self.m = self.X.shape[0]\n",
      "        thetas = self.init_thetas(0.01)\n",
      "        #print(\"init thetas\")\n",
      "        #print(thetas)\n",
      "        \n",
      "        #self.backprop(thetas)\n",
      "        \n",
      "        print (self.test_grad(thetas))\n",
      "        #self.test_grad(thetas)\n",
      "        #if not(self.test_grad(thetas)):\n",
      "        #    print(\"Gradient test failed\")\n",
      "        #    return None\n",
      "        \n",
      "        thetas, fmin, info = fmin_l_bfgs_b(self.costgrad, thetas,\n",
      "                                           #callback=self.callback,\n",
      "                                           factr=1e7)\n",
      "        self.count = self.count + 1\n",
      "        #print(self.count, end=\", \")\n",
      "        #print(datetime.now())\n",
      "        #print(\"final thetas\")\n",
      "        #print(thetas)\n",
      "        model = NeuralNetClassifier(self.domain, thetas) # self.domain se nalozi pred klicem fukncije fit\n",
      "        model.h = self.h\n",
      "        model.shape_thetas = self.shape_thetas\n",
      "        return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print (xor_data.Y)\n",
      "ann = NeuralNetLearner(xor_arch, lambda_=0.0001)\n",
      "\n",
      "model = ann(xor_data)\n",
      "model(xor_data.X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "grad diff\n",
        "1.41375242143e-07\n",
        "---------------------------------\n",
        "False\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "array([1, 0, 1, 0])"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TEST!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "atts = [Orange.data.ContinuousVariable('x1'), Orange.data.ContinuousVariable('x2')]\n",
      "class_var = Orange.data.DiscreteVariable('Class_1', values=(0, 1))\n",
      "dbg_domain = Orange.data.Domain(attributes=atts, class_vars=class_var)\n",
      "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
      "and_data = Orange.data.Table(dbg_domain, X, np.array([0]*3+[1]).T)\n",
      "xor_data = Orange.data.Table(dbg_domain, X, np.array([0]+[1]*2+[0]).T)\n",
      "xor_arch = (2,2,2)\n",
      "xor_thetas = np.array([-30, 10, 20, -20, 20, -20, 10, -20, -20])\n",
      "print (and_data)\n",
      "print (xor_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0.000, 0.000 | 0],\n",
        " [0.000, 1.000 | 0],\n",
        " [1.000, 0.000 | 0],\n",
        " [1.000, 1.000 | 1]\n",
        "[[0.000, 0.000 | 0],\n",
        " [0.000, 1.000 | 1],\n",
        " [1.000, 0.000 | 1],\n",
        " [1.000, 1.000 | 0]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(datetime.now())\n",
      "ann = NeuralNetLearner(iris_arch, lambda_=0.00001)\n",
      "res = Orange.evaluation.CrossValidation(iris_dbg_data, [ann], k=10)\n",
      "print(\"\")\n",
      "print(datetime.now())\n",
      "LogLoss(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2015-05-16 16:18:58.120137\n",
        "grad diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7.25028841089e-06\n",
        "---------------------------------\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:58.359151\n",
        "grad diff\n",
        "1.76522246855e-06\n",
        "---------------------------------\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:58.476158\n",
        "grad diff\n",
        "5.97462066207e-06\n",
        "---------------------------------\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:58.577163\n",
        "grad diff\n",
        "8.30957311942e-06\n",
        "---------------------------------\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:58.688170\n",
        "grad diff\n",
        "1.22683719455e-05\n",
        "---------------------------------\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:58.945184\n",
        "grad diff\n",
        "7.43487427717e-06\n",
        "---------------------------------\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:59.088193\n",
        "grad diff\n",
        "2.33958180909e-05\n",
        "---------------------------------\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:59.197199\n",
        "grad diff\n",
        "5.45340676974e-06\n",
        "---------------------------------\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:59.341207\n",
        "grad diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4.38194053543e-06\n",
        "---------------------------------\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:59.559220\n",
        "grad diff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.55727267931e-06\n",
        "---------------------------------\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 16:18:59.748230\n",
        "\n",
        "2015-05-16 16:18:59.749230\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 429,
       "text": [
        "array([ 0.1084216407582395])"
       ]
      }
     ],
     "prompt_number": 429
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(datetime.now())\n",
      "ann = NeuralNetLearner((93, 20, 9), lambda_=0.00001)\n",
      "#model = ann(train_data)\n",
      "#predictions = model(test_data.X, ret=Model.Probs)\n",
      "\n",
      "res = Orange.evaluation.CrossValidation(train_data, [ann], k=8)\n",
      "\n",
      "print(\"\")\n",
      "print(datetime.now())\n",
      "#predictions\n",
      "LogLoss(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2015-05-16 17:30:07.604338\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 17:45:36.695479\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 18:02:29.524409\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 18:16:29.217437\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 18:30:33.770743\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 18:45:20.559464\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 18:53:01.408823\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 19:03:34.436030\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2015-05-16 19:16:45.092253\n",
        "\n",
        "2015-05-16 19:16:45.130255\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 435,
       "text": [
        "array([ 0.5886610522480696])"
       ]
      }
     ],
     "prompt_number": 435
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_93_20_9_00001 =  1.5330132438807742\n",
      "cv_93_20_9_00001_softmax = 0.5861296880013968\n",
      "cv_93_10_10_9_00001_soft = 0.5956303653531793\n",
      "cv_93_20_9_00001_soft_01 = 0.5886610522480696"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 436
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = read_data('train.csv')\n",
      "test_data = read_data('test.csv', read_class=False)\n",
      "\n",
      "norm = Normalizer()\n",
      "norm.fit(train_data.X)\n",
      "train_data = Table(train_data.domain, norm.transform(train_data.X), train_data.Y)\n",
      "test_data = Table(test_data.domain, norm.transform(test_data.X), test_data.Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 431
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data.X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 199,
       "text": [
        "(11878, 93)"
       ]
      }
     ],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "atts = [ContinuousVariable('x1'), ContinuousVariable('x2')]\n",
      "class_var = DiscreteVariable('Class_1', values=(0, 1))\n",
      "dbg_domain = Domain(attributes=atts, class_vars=class_var)\n",
      "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
      "xor_data = Table(dbg_domain, X, np.array([0]+[1]*2+[0]).T)\n",
      "#xor_data = Orange.data.Table(\"xor\")\n",
      "xor_data = Table(dbg_domain, xor_data.X, xor_data.Y)\n",
      "xor_arch = (2,4,2)\n",
      "xor_thetas = np.array([-30, 10, 20, -20, 20, -20, 10, -20, -20])\n",
      "\n",
      "iris = Orange.data.Table(\"iris\")\n",
      "class_var = DiscreteVariable('Class_1', values=(i for i in range(len(iris.domain.class_var.values))))\n",
      "iris_dbg_domain = Domain(iris.domain.attributes, class_var)\n",
      "\n",
      "iris_norm = Normalizer()\n",
      "iris_norm.fit(iris.X)\n",
      "iris_dbg_data = Table(iris_dbg_domain, iris_norm.transform(iris.X), iris.Y)\n",
      "iris_arch = (4, 4, 3)\n",
      "iris_dbg_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[[0.8, 0.6, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.3, 0.0 | 0],\n",
        " [0.8, 0.6, 0.2, 0.0 | 0],\n",
        " ...\n",
        "]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#and\n",
      "and_theta = np.array([-30, 20, 20])\n",
      "for x in add_ones(and_data.X):\n",
      "    print(\"%d %d -> %3.1f\" % (x[1], x[2], g(theta, x)))\n",
      "print(\"--------------------\")\n",
      "    \n",
      "#xor\n",
      "a1 = add_ones(X)\n",
      "theta1 = np.array([[-30, 10], [20, -20], [20, -20]])\n",
      "z2 = a1.dot(theta1)\n",
      "a2 = add_ones(g(theta1, a1))\n",
      "theta2 = np.array([[10], [-20], [-20]])\n",
      "z3 = a2.dot(theta2)\n",
      "a3 = g(theta2, a2)\n",
      "y = a3\n",
      "\n",
      "for xi, yi in zip(X, y):\n",
      "    print(\"%d %d -> %3.1f\" % (xi[0], xi[1], yi))\n",
      "\n",
      "# ann = NeuralNetLearner((data.X.shape[1], 5, 4), lambda_=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0 -> 0.0\n",
        "0 1 -> 0.0\n",
        "1 0 -> 0.0\n",
        "1 1 -> 1.0\n",
        "--------------------\n",
        "0 0 -> 0.0\n",
        "0 1 -> 1.0\n",
        "1 0 -> 1.0\n",
        "1 1 -> 0.0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for xi, yi in zip(X, y):\n",
      "    print(\"%d %d -> %3.1f\" % (xi[0], xi[1], yi))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0 -> 1.0\n",
        "0 1 -> 0.0\n",
        "1 0 -> 0.0\n",
        "1 1 -> 1.0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[0.147, 0.000, 0.000, 0.000, 0.000, ... | 0]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fo = open(\"Napovedi\\\\NN_log_93-10-10-9_lmbd0.00001_soft_th_init0.01.csv\", \"wt\")\n",
      "fo.write(\"id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9\\n\")\n",
      "\n",
      "for i, probs in zip(range(1, len(predictions)+1), predictions):\n",
      "    fo.write(\"%d\" % (i))\n",
      "    for p in probs:\n",
      "        fo.write(\",%.16f\" % (p))\n",
      "    fo.write(\"\\n\")\n",
      "fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 433
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}