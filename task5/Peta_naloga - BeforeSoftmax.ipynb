{
 "metadata": {
  "name": "",
  "signature": "sha256:c068689a110de8b9e52046da209737414366f6adb61420f53c8b374e08dbe06d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.set_printoptions(suppress=True, precision=16)\n",
      "\n",
      "import Orange\n",
      "from Orange.classification import Model, Learner\n",
      "from Orange.data import Table, Domain, DiscreteVariable, ContinuousVariable\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NUM_OF_COLS = 94\n",
      "NUM_OF_CLASSSES = 9\n",
      "E = 1e-8"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def g(theta, x):\n",
      "    return 1. / (1 + np.exp(-x.dot(theta)))\n",
      "\n",
      "def softmax(self, a, theta):\n",
      "        z = a.dot(theta)\n",
      "        ps = np.exp(z - np.max(z, axis=1)[:,None])\n",
      "        ps /= np.sum(ps, axis=1)[:, None]\n",
      "        return s\n",
      "\n",
      "def add_ones(X):\n",
      "    return np.column_stack((np.ones(len(X)), X))\n",
      "\n",
      "def read_data(fname, read_class=True):\n",
      "    X = np.loadtxt(fname, delimiter=',', skiprows=1, usecols=range(1,NUM_OF_COLS))\n",
      "    atts = [ContinuousVariable(\"feature_\" + str(i)) for i in range(1, NUM_OF_COLS)]\n",
      "    class_var = DiscreteVariable(\"class\", values=[i for i in range(NUM_OF_CLASSSES)])\n",
      "    domain = Domain(atts, class_var)\n",
      "    if read_class:\n",
      "        y = np.loadtxt(fname, delimiter=',', skiprows=1, usecols=range(NUM_OF_COLS,NUM_OF_COLS+1), dtype=np.dtype(str))\n",
      "        y = np.array([float(c[-2]) - 1 for c in y])\n",
      "    else:\n",
      "        y = np.zeros(len(X))\n",
      "    return Table(domain, X, y)\n",
      "\n",
      "def numerical_grad(f, params, epsilon):\n",
      "    num_grad = np.zeros(len(params))\n",
      "    perturb = np.zeros(len(params))\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad\n",
      "\n",
      "def indicator(Y, k):\n",
      "    ind = np.zeros(Y.shape)\n",
      "    for i in range(k):\n",
      "        ind_i = Y == i\n",
      "        ind = np.column_stack((ind, ind_i.astype(int)))\n",
      "    ind = ind[:,1:]\n",
      "    return ind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LogLoss(Orange.evaluation.scoring.Score):\n",
      "    def log_loss(self, y, P):\n",
      "        m = P.shape[0]\n",
      "        \n",
      "        Y = np.zeros(P.shape)\n",
      "        for i in range(Y.shape[1]):\n",
      "            Y[y == i,i] = 1\n",
      "            \n",
      "        almost_ones = np.ones(P.shape) - E\n",
      "        almost_zeros = np.zeros(P.shape) + E\n",
      "        \n",
      "        log = np.log(np.maximum(np.minimum(P, almost_ones), almost_zeros))\n",
      "        \n",
      "        return (-1/m) * np.sum(Y * log)\n",
      "        \n",
      "    def compute_score(self, results):\n",
      "        domain = results.domain\n",
      "        n_classes = len(domain.class_var.values)\n",
      "        \n",
      "        scores = []\n",
      "        \n",
      "        for ps in results.probabilities:\n",
      "            scores.append(metrics.log_loss(results.actual, ps))\n",
      "            \n",
      "        return np.array(scores)\n",
      "\n",
      "class ParameterFit:\n",
      "    def __init__(self, learner, param_set):\n",
      "        self.learner = learner\n",
      "        self.param_set = param_set\n",
      "    def __call__(self, data, return_learner=False):\n",
      "        learners = [self.learner(p) for p in self.param_set]\n",
      "        res = Orange.evaluation.CrossValidation(data, learners, k=10)\n",
      "        scores = LogLoss(res).tolist()\n",
      "        #print(self.param_set)\n",
      "        #print(scores)\n",
      "        min_i = scores.index(np.min(scores))\n",
      "        best_p = self.param_set[min_i]\n",
      "        best_learner = self.learner(best_p)\n",
      "        if return_learner:\n",
      "            return best_learner\n",
      "        return best_learner(data)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetClassifier(Orange.classification.Model):\n",
      "    \"\"\"Neural network classifier based on a set of binary classifiers.\"\"\"\n",
      "    def __init__(self, domain, thetas):\n",
      "        super().__init__(domain)\n",
      "        self.thetas = thetas  # model parameters\n",
      "\n",
      "    def predict(self, X):\n",
      "        return self.h(X, self.thetas)\n",
      "        #y_hat = np.ravel(self.h(X, self.thetas))\n",
      "        # following works only for binary classifiers, correct it for multiclass\n",
      "        #return np.vstack((1-y_hat, y_hat)).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetLearner(Orange.classification.Learner):\n",
      "    def __init__(self, arch, lambda_=1e-5):\n",
      "        super().__init__()\n",
      "        self.arch = arch\n",
      "        self.lambda_ = lambda_\n",
      "        self.name = \"ann\"\n",
      "        self.count = 0\n",
      "\n",
      "        self.theta_shape = np.array([(arch[i]+1, arch[i+1])\n",
      "                                     for i in range(len(arch)-1)])\n",
      "        ind = np.array([s1*s2 for s1, s2 in self.theta_shape])\n",
      "        self.theta_ind = np.cumsum(ind[:-1])\n",
      "        self.theta_len = sum(ind)\n",
      "\n",
      "    def shape_thetas(self, thetas):\n",
      "        t = np.split(thetas, self.theta_ind)\n",
      "        return [t[i].reshape(shape) for i, shape in enumerate(self.theta_shape)]\n",
      "\n",
      "    def test(self, a):\n",
      "        thetas = np.array([-30, 10, 20, -20, 20, -20, -10, 20, 20])\n",
      "        print(self.h(a, thetas))\n",
      "\n",
      "    def init_thetas(self, epsilon=1e-5):\n",
      "        #np.random.seed(10)\n",
      "        print (\"SELF.THETA_LEN:\" )\n",
      "        print (self.theta_len)\n",
      "        return np.random.rand(self.theta_len) * 2 * epsilon - epsilon\n",
      "        #return np.array([-3, 1, 2, -2, 2, -2, 1, -2, -2])\n",
      "    \n",
      "    def test_grad(self, thetas):\n",
      "        ag = self.costgrad(thetas)[1]\n",
      "        ng = self.grad_approx(thetas, 1e-4)\n",
      "        #print(\"ag\")\n",
      "        #print(ag)\n",
      "        #print(\"ng\")\n",
      "        #print(ng)\n",
      "        print(\"grad diff\")\n",
      "        print(np.sum((ag - ng)**2))\n",
      "        print(\"---------------------------------\")\n",
      "        return np.sum((ag - ng)**2) < E\n",
      "\n",
      "    def grad_approx(self, thetas, e=1e-4):\n",
      "        return np.array([(self.costgrad(thetas+eps, return_cost=True) - self.costgrad(thetas-eps, return_cost=True))/(2*e)\n",
      "                         for eps in np.identity(len(thetas)) * e])\n",
      "    \n",
      "    def shape_y(self, y):\n",
      "        values = np.unique(y)\n",
      "        y1 = np.zeros((len(y), len(values)))\n",
      "        for v in values:\n",
      "            y1[y == v, v] = 1\n",
      "        return y1\n",
      "\n",
      "    def costgrad(self, thetas_flat, return_cost=False):\n",
      "        # use matrix and vector operations. could be written in a single line\n",
      "        # use self.m as stored by the fit function\n",
      "        y = self.shaped_y\n",
      "        X = self.X\n",
      "        m = self.m\n",
      "        \n",
      "        \"\"\"print (\"thetas: \")\n",
      "        print (thetas_flat)\n",
      "        print (\"X: \")\n",
      "        print (X)\n",
      "        print (\"y: \")\n",
      "        print (y)\n",
      "        print (\"m: \")\n",
      "        print (m)\"\"\"\n",
      "        \n",
      "        ################################ COST ################################\n",
      "        \n",
      "        term1 = self.h(X, thetas_flat) - y\n",
      "        #print (\"activations: \")\n",
      "        #print (self.activations)\n",
      "        #print (\"h: \")\n",
      "        #print (self.h(X, thetas_flat))\n",
      "        #print (\"h - y: \")\n",
      "        #print (term1)\n",
      "        term1 = term1 ** 2\n",
      "        term1 = sum(sum(term1))\n",
      "        term1 /= (2*m)\n",
      "        #print(\"term1\")\n",
      "        #print(term1)\n",
      "        \n",
      "        # calculate regularisation term\n",
      "        thetas = self.shape_thetas(thetas_flat)        \n",
      "        term2 = 0        \n",
      "        for theta in thetas:\n",
      "            theta = theta[1:]\n",
      "            term2 += sum(sum(theta ** 2))\n",
      "        term2 *= (self.lambda_/2)\n",
      "        #print(\"term2\")\n",
      "        #print(term2)\n",
      "        \n",
      "        \n",
      "        cost = term1 + term2\n",
      "        \n",
      "        print(\"cost: \")\n",
      "        print (cost)\n",
      "        \n",
      "        if return_cost:\n",
      "            return cost\n",
      "        \n",
      "        ################################ GRAD ################################\n",
      "        \n",
      "        # 1. Perform a feedforward pass, computing the activations\n",
      "        #self.h(X, thetas_flat) # DONE in cost part\n",
      "        \n",
      "        # 2. For the output layer (layer nl), set error\n",
      "        a = self.activations[-1]\n",
      "        fz = a * (1 - a)\n",
      "        deltas = [-(y - a) * fz]\n",
      "        \n",
      "        print (\"deltas:\")\n",
      "        print (deltas)\n",
      "        # 3. For l = nl \u2212 1, nl \u2212 2, nl \u2212 3, . . . , 2 set errors and calculate gradient\n",
      "        thetas = self.shape_thetas(thetas_flat)\n",
      "        grads_flat = []        \n",
      "        # activations without last one and thetas must be of same shapes\n",
      "        print (\"FOR -----------------------\")\n",
      "        for a, theta_bias in zip(reversed(self.activations[:-1]), reversed(thetas)):\n",
      "            theta = theta_bias[1:] # remove bias\n",
      "            fz = a * (1 - a)            \n",
      "            dl1 = deltas[0]\n",
      "            \n",
      "            deltas = [dl1.dot(theta.T) * fz] + deltas\n",
      "            print (\"deltas:\")\n",
      "            print (deltas)\n",
      "            # 4. Compute the desired partial derivatives for all thetas\n",
      "            # add ones to activations to propagate bias to lower levels!!!!\n",
      "            grad_theta = add_ones(a).T.dot(dl1)\n",
      "            grads_flat = grad_theta.ravel().tolist() + grads_flat\n",
      "            print (\"grad: \")\n",
      "            print (grads_flat)\n",
      "        grads_flat = np.array(grads_flat)\n",
      "        \n",
      "        # calculate regularisation term\n",
      "        term2 = 0        \n",
      "        for theta in thetas:\n",
      "            theta = theta[1:]\n",
      "            term2 += sum(sum(theta))\n",
      "        term2 *= (self.lambda_)\n",
      "        \n",
      "        grad = (1/m) * grads_flat + term2\n",
      "        print (\"grad:\")\n",
      "        print (grad)\n",
      "        return (grad)\n",
      "        \n",
      "        return cost, grad\n",
      "\n",
      "    def h(self, a, thetas):\n",
      "        \"\"\"feed forward, prediction\"\"\"\n",
      "        thetas = self.shape_thetas(thetas)\n",
      "        self.activations = [a]\n",
      "        for theta in thetas:\n",
      "            a = g(theta, add_ones(a))\n",
      "            self.activations.append(a)\n",
      "        return a       \n",
      "\n",
      "    def fit(self, X, y, W=None):\n",
      "        self.X, self.y = X, y\n",
      "        self.shaped_y = self.shape_y(self.y)\n",
      "        self.m = self.X.shape[0]\n",
      "        thetas = self.init_thetas(1)\n",
      "        print (\"THE THETAS\")\n",
      "        print (thetas)\n",
      "        #print(\"init thetas\")\n",
      "        #print(thetas)\n",
      "        \n",
      "        #self.backprop(thetas)\n",
      "        \n",
      "        #self.test_grad(thetas)\n",
      "        #if not(self.test_grad(thetas)):\n",
      "        #    print(\"Gradient test failed\")\n",
      "        #    return None\n",
      "        \n",
      "        thetas, fmin, info = fmin_l_bfgs_b(self.costgrad, thetas,\n",
      "                                           #callback=self.callback,\n",
      "                                           factr=10)\n",
      "        self.count = self.count + 1\n",
      "        print(self.count, end=\", \")\n",
      "        #print(\"final thetas\")\n",
      "        #print(thetas)\n",
      "        model = NeuralNetClassifier(self.domain, thetas) # self.domain se nalozi pred klicem fukncije fit\n",
      "        model.h = self.h\n",
      "        model.shape_thetas = self.shape_thetas\n",
      "        return model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ann = NeuralNetLearner((2, 2, 2), lambda_=0.00001)\n",
      "model = ann(xor_data)\n",
      "model(xor_data.X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "failed to initialize intent(inout|inplace|cache) array -- input must be array but got (null)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-82-7f77dbc97c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxor_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxor_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/luka/Documents/Programs/orange3/Orange/classification/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_multiclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_multiclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-79-5fac5dfa7ee3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, W)\u001b[0m\n\u001b[1;32m    176\u001b[0m         thetas, fmin, info = fmin_l_bfgs_b(self.costgrad, thetas,\n\u001b[1;32m    177\u001b[0m                                            \u001b[0;31m#callback=self.callback,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                            factr=10)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 185\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    186\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    187\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    302\u001b[0m         _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    303\u001b[0m                        \u001b[0mpgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                        isave, dsave)\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mtask_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'FG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: failed to initialize intent(inout|inplace|cache) array -- input must be array but got (null)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SELF.THETA_LEN:\n",
        "12\n",
        "THE THETAS\n",
        "[ 0.771122129396842   0.8434911936095284  0.6689933567903588\n",
        "  0.2034856931485232 -0.3565075696070901 -0.6927837461764641\n",
        " -0.3241733734573051 -0.2005122119317764 -0.4804720645127414\n",
        "  0.7542296929892507  0.4047549514897837  0.1152425712331395]\n",
        "cost: \n",
        "0.26095239217\n",
        "deltas:\n",
        "[array([[-0.1429075991967226,  0.1437164882298204],\n",
        "       [ 0.0967333000354477, -0.1028519430320482],\n",
        "       [ 0.0954192154028761, -0.0891418826873377],\n",
        "       [-0.1450697252592812,  0.1446968171203022]])]\n",
        "FOR -----------------------\n",
        "deltas:\n",
        "[array([[ 0.0382854948855797, -0.0086820465676753],\n",
        "       [-0.0297173659802887,  0.0067864820649911],\n",
        "       [-0.0175097951514608,  0.0054516034533012],\n",
        "       [ 0.0337829385476248, -0.0101877396320054]]), array([[-0.1429075991967226,  0.1437164882298204],\n",
        "       [ 0.0967333000354477, -0.1028519430320482],\n",
        "       [ 0.0954192154028761, -0.0891418826873377],\n",
        "       [-0.1450697252592812,  0.1446968171203022]])]\n",
        "grad: \n",
        "[-0.09582480901768015, 0.09641947963073667, -0.07071163660192897, 0.07237653933414777, -0.06253579893636518, 0.06423926694280016]\n",
        "deltas:\n",
        "[array([[ 0., -0.],\n",
        "       [-0.,  0.],\n",
        "       [-0.,  0.],\n",
        "       [ 0., -0.]]), array([[ 0.0382854948855797, -0.0086820465676753],\n",
        "       [-0.0297173659802887,  0.0067864820649911],\n",
        "       [-0.0175097951514608,  0.0054516034533012],\n",
        "       [ 0.0337829385476248, -0.0101877396320054]]), array([[-0.1429075991967226,  0.1437164882298204],\n",
        "       [ 0.0967333000354477, -0.1028519430320482],\n",
        "       [ 0.0954192154028761, -0.0891418826873377],\n",
        "       [-0.1450697252592812,  0.1446968171203022]])]\n",
        "grad: \n",
        "[0.024841272301455077, -0.006631700681388467, 0.016273143396164053, -0.004736136178704178, 0.004065572567336154, -0.003401257567014291, -0.09582480901768015, 0.09641947963073667, -0.07071163660192897, 0.07237653933414777, -0.06253579893636518, 0.06423926694280016]\n",
        "grad:\n",
        "[ 0.0062164875042173 -0.0016517557414936  0.0040744552778946\n",
        " -0.0011778646158225  0.0010225625706876 -0.0008441449629\n",
        " -0.0239500328255665  0.0241110393365377 -0.0176717397216287\n",
        "  0.0181003042623905 -0.0156277803052377  0.0160659861645536]\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(datetime.now())\n",
      "ann = NeuralNetLearner(iris_arch, lambda_=0.00001)\n",
      "res = Orange.evaluation.CrossValidation(iris_dbg_data, [ann], k=10)\n",
      "print(\"\")\n",
      "print(datetime.now())\n",
      "LogLoss(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2015-05-15 11:26:30.893806\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", \n",
        "2015-05-15 11:26:33.139934\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "array([ 0.1051606343286239])"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(datetime.now())\n",
      "ann = NeuralNetLearner((93, 20, 9), lambda_=0.00001)\n",
      "#model = ann(train_data)\n",
      "#predictions = model(test_data.X, ret=Model.Probs)\n",
      "\n",
      "res = Orange.evaluation.CrossValidation(train_data, [ann], k=10)\n",
      "\n",
      "print(\"\")\n",
      "print(datetime.now())\n",
      "#predictions\n",
      "LogLoss(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2015-05-15 12:14:13.905984\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", 10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ", "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2015-05-15 15:24:52.860722\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 249,
       "text": [
        "array([ 1.5533649363765856])"
       ]
      }
     ],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "LogLoss(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 252,
       "text": [
        "array([ 1.5330132438807742])"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = read_data('train.csv')\n",
      "test_data = read_data('test.csv', read_class=False)\n",
      "\n",
      "norm = Normalizer()\n",
      "norm.fit(train_data.X)\n",
      "train_data = Table(train_data.domain, norm.transform(train_data.X), train_data.Y)\n",
      "test_data = Table(test_data.domain, norm.transform(test_data.X), test_data.Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data.X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 199,
       "text": [
        "(11878, 93)"
       ]
      }
     ],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "atts = [ContinuousVariable('x1'), ContinuousVariable('x2')]\n",
      "class_var = DiscreteVariable('Class_1', values=(0, 1))\n",
      "dbg_domain = Domain(attributes=atts, class_vars=class_var)\n",
      "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
      "xor_data = Table(dbg_domain, X, np.array([0]+[1]*2+[0]).T)\n",
      "#xor_data = Orange.data.Table(\"xor\")\n",
      "xor_data = Table(dbg_domain, xor_data.X, xor_data.Y)\n",
      "xor_arch = (2,2,2)\n",
      "xor_thetas = np.array([-30, 10, 20, -20, 20, -20, 10, -20, -20])\n",
      "\n",
      "iris = Orange.data.Table(\"iris\")\n",
      "class_var = DiscreteVariable('Class_1', values=(i for i in range(len(iris.domain.class_var.values))))\n",
      "iris_dbg_domain = Domain(iris.domain.attributes, class_var)\n",
      "\n",
      "iris_norm = Normalizer()\n",
      "iris_norm.fit(iris.X)\n",
      "iris_dbg_data = Table(iris_dbg_domain, iris_norm.transform(iris.X), iris.Y)\n",
      "iris_arch = (4, 4, 3)\n",
      "iris_dbg_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[[0.8, 0.6, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.2, 0.0 | 0],\n",
        " [0.8, 0.5, 0.3, 0.0 | 0],\n",
        " [0.8, 0.6, 0.2, 0.0 | 0],\n",
        " ...\n",
        "]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#and\n",
      "and_theta = np.array([-30, 20, 20])\n",
      "for x in add_ones(and_data.X):\n",
      "    print(\"%d %d -> %3.1f\" % (x[1], x[2], g(theta, x)))\n",
      "print(\"--------------------\")\n",
      "    \n",
      "#xor\n",
      "a1 = add_ones(X)\n",
      "theta1 = np.array([[-30, 10], [20, -20], [20, -20]])\n",
      "z2 = a1.dot(theta1)\n",
      "a2 = add_ones(g(theta1, a1))\n",
      "theta2 = np.array([[10], [-20], [-20]])\n",
      "z3 = a2.dot(theta2)\n",
      "a3 = g(theta2, a2)\n",
      "y = a3\n",
      "\n",
      "for xi, yi in zip(X, y):\n",
      "    print(\"%d %d -> %3.1f\" % (xi[0], xi[1], yi))\n",
      "\n",
      "# ann = NeuralNetLearner((data.X.shape[1], 5, 4), lambda_=0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0 -> 0.0\n",
        "0 1 -> 0.0\n",
        "1 0 -> 0.0\n",
        "1 1 -> 1.0\n",
        "--------------------\n",
        "0 0 -> 0.0\n",
        "0 1 -> 1.0\n",
        "1 0 -> 1.0\n",
        "1 1 -> 0.0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for xi, yi in zip(X, y):\n",
      "    print(\"%d %d -> %3.1f\" % (xi[0], xi[1], yi))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0 -> 1.0\n",
        "0 1 -> 0.0\n",
        "1 0 -> 0.0\n",
        "1 1 -> 1.0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[0.147, 0.000, 0.000, 0.000, 0.000, ... | 0]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fo = open(\"Napovedi\\\\NN_log_93-20-9_lmbd0.00001_biasB.csv\", \"wt\")\n",
      "fo.write(\"id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9\\n\")\n",
      "\n",
      "for i, probs in zip(range(1, len(predictions)+1), predictions):\n",
      "    fo.write(\"%d\" % (i))\n",
      "    for p in probs:\n",
      "        fo.write(\",%.16f\" % (p))\n",
      "    fo.write(\"\\n\")\n",
      "fo.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}