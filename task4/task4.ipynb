{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "%matplotlib inline\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "import Orange"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Classes And Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class IO:\n",
      "    def read(fn, train=True):\n",
      "        X = np.genfromtxt(fn, delimiter = ',', usecols = np.arange(0, 93), skiprows = 1, dtype = float)\n",
      "        # converts first line into int and saves it\n",
      "        ids = X[:, 0].copy().astype(int)\n",
      "        # changes first line to ones\n",
      "        X[:, 0] = np.ones(X.shape[0])\n",
      "        if(train):\n",
      "            y = np.genfromtxt(fn, delimiter = ',', usecols = {94}, skiprows = 1, dtype = str)\n",
      "            # converts classes from string to float numbers\n",
      "            yn = np.zeros(len(y))\n",
      "            i = 0\n",
      "            for el in y:\n",
      "                yn[i] = float(el[6])\n",
      "                i += 1   \n",
      "            y = yn\n",
      "            return X, y\n",
      "        else:\n",
      "            return X, ids\n",
      "    def write(res, ids):\n",
      "        fc = np.hstack((['id'], ids))\n",
      "        oc = np.vstack((['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'], res))\n",
      "        sres = np.column_stack((fc, oc))\n",
      "        np.savetxt('results.csv', sres, delimiter=\",\", fmt=\"%s\") \n",
      "class LogicalRegression:\n",
      "    def __init__(self, lam):\n",
      "        self.lam = lam\n",
      "    def __call__(self, X, y):\n",
      "        init_theta = np.zeros(X.shape[1]).T\n",
      "        return fmin_l_bfgs_b(LogicalRegression.cost, init_theta, LogicalRegression.gradient, args=(X, y, self.lam))[0]\n",
      "    def sigmoid(theta, x):\n",
      "        return 1. / (1 + np.exp(-x.dot(theta)))\n",
      "    \n",
      "    #gradient for compution in l_bfsg\n",
      "    def gradient(theta, X, y, lam):\n",
      "        h = LogicalRegression.sigmoid(theta, X)\n",
      "        g1 = (1. / X.shape[0]) * (h - y).T.dot(X[:, 0])\n",
      "        g2 = (1. / X.shape[0]) * (((h - y).dot(X[:, 1:X.shape[1]])) + lam * theta[1:])\n",
      "        return np.hstack((g1, g2)).T\n",
      "\n",
      "    #cost function for compution in l_bfsg\n",
      "    def cost(theta, X, y, lam):\n",
      "        h = LogicalRegression.sigmoid(theta, X)\n",
      "        return (1. / X.shape[0]) * ((-y.T.dot(np.log(h + 1e-10) )) - (1 - y.T).dot(np.log(1. - h + 1e-10))) + (lam / (2. * X.shape[0])) * (theta[1:].T.dot(theta[1:]))\n",
      "\n",
      "class SoftmaxRegression:\n",
      "    def __init__(self, lam):\n",
      "        self.lam = lam\n",
      "    def __call__(self, X, y):\n",
      "        init_thetas=np.zeros(len(np.unique(y)) * X.shape[1])\n",
      "        return fmin_l_bfgs_b(SoftmaxRegression.cost, init_thetas, SoftmaxRegression.gradient, args=(X, y, self.lam, np.unique(y)))[0]\n",
      "    def cost(theta, X, y, lam, c):\n",
      "        #pd = 1 / sum(np.exp(theta.dot(X.T)))\n",
      "        #for k in c:\n",
      "            \n",
      "        #print (pd)\n",
      "        #pd.shape\n",
      "        #print (y)\n",
      "        #return (np.array(y == 1.0).astype(int))\n",
      "        t = theta.reshape(len(c), X.shape[1])\n",
      "        h = np.exp(X.dot(t.T))\n",
      "        p = h / np.sum(h, axis = 1)[:, None]\n",
      "        # elements with ones and zeros if calculation is for them\n",
      "        gt = np.array([np.array(y == el).astype(int) for el in c])\n",
      "        \n",
      "        # by = np.eye(len(c))[y.ravel().astype(int)]\n",
      "        \n",
      "        tc = (-np.sum(np.log(p) * gt.T) + lam * theta.dot(theta) / 2.) / X.shape[0]\n",
      "        #co = gt.dot(np.log(p).T)\n",
      "        #print (co)\n",
      "        #tc = -(np.sum(co) / X.shape[1])\n",
      "        #print (test)\n",
      "        return tc\n",
      "    \n",
      "    def gradient(theta, X, y, lam, c):\n",
      "        t = theta.reshape(len(c), X.shape[1])\n",
      "        h = np.exp(X.dot(t.T))\n",
      "        p = h / np.sum(h, axis = 1)[:, None]\n",
      "        gt = np.array([np.array(y == el).astype(int) for el in c])\n",
      "        grad = (X.T.dot(p - gt.T).T + lam * t) / X.shape[0]\n",
      "        #grad = np.array(-(gt - p).dot(X) / X.shape[1])\n",
      "        return grad.ravel()\n",
      "    \n",
      "class OneVsAll:\n",
      "    def __init__(self, f):\n",
      "        self.f = f\n",
      "    def __call__(self, X, y):\n",
      "        t = []\n",
      "        # all classes\n",
      "        c = np.unique(y)\n",
      "        # changes specific class to 1 and others to 0\n",
      "        for el in c:\n",
      "            yn = y.copy()\n",
      "            yn[y == el] = 1\n",
      "            yn[y != el] = 0\n",
      "            t = np.append(t, self.f(X, yn))\n",
      "        return t.reshape((len(t) / X.shape[1]), X.shape[1])\n",
      "    \n",
      "class OneVsOne:\n",
      "    def __init__(self, f):\n",
      "        self.f = f\n",
      "    def __call__(self, X, y):\n",
      "        t = []\n",
      "        # all classes\n",
      "        c = np.unique(y)\n",
      "        # changes specific class to 1 and others to 0\n",
      "        for i in range(len(c) - 1):\n",
      "            for j in range(i + 1, len(c)):\n",
      "                #print (j)\n",
      "                yn = y.copy()\n",
      "                # finds elements in array for this comparison for later vector/matrix cut\n",
      "                cut = np.logical_or(yn == c[i], yn == c[j])\n",
      "                # sets desirable elements to 0 or 1\n",
      "                yn[y == c[i]] = 1\n",
      "                yn[y == c[j]] = 0\n",
      "                yn = yn[cut]\n",
      "                Xn = X[cut]\n",
      "                t = np.append(t, self.f(Xn, yn))\n",
      "        #return t\n",
      "        return t.reshape((len(t) / ((len(c) * (len(c) - 1)) / 2)), (len(c) * (len(c) - 1)) / 2)\n",
      "    def result(ovor, l):\n",
      "        res = np.zeros((ovor.shape[0], l))\n",
      "        k = 0\n",
      "        for i in range(l - 1):\n",
      "            for j in range(i + 1, l):\n",
      "                fc = ovor[:,k].copy()\n",
      "                fc[ovor[:,k] > 0.5] = 1\n",
      "                fc[ovor[:,k] <= 0.5] = 0\n",
      "                lc = ovor[:,k].copy()\n",
      "                lc[ovor[:,k] > 0.5] = 0\n",
      "                lc[ovor[:,k] <= 0.5] = 1\n",
      "                res[:,i] = res[:,i] + fc\n",
      "                res[:,j] = res[:,j] + lc\n",
      "                k += 1\n",
      "        return res / ovor.shape[1]\n",
      "\n",
      "init_thetas=np.zeros(len(np.unique(y)) * X.shape[1])\n",
      "ng = SoftmaxRegression.cost(init_thetas, X, y, 0.01, np.unique(y))\n",
      "ag = SoftmaxRegression.gradient(init_thetas, X, y, 0.01, np.unique(y))\n",
      "ng"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "2.1972245773362213"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Softmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = IO.read('train.csv')\n",
      "lam = 0.01\n",
      "lr = SoftmaxRegression(lam)\n",
      "thetas = lr(X, y)\n",
      "t = thetas.reshape(len(np.unique(y)), X.shape[1])\n",
      "Xt, ids = IO.read('test.csv', False)\n",
      "# calculate results on test data\n",
      "res = LogicalRegression.sigmoid(t.T, Xt)\n",
      "IO.write(res, ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "One vs All"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = IO.read('train.csv')\n",
      "lam = 0.01\n",
      "lr = LogicalRegression(lam)\n",
      "ova = OneVsAll(lr)\n",
      "thetas = ova(X, y)\n",
      "Xt, ids = IO.read('test.csv', False)\n",
      "# calculate results on test data\n",
      "res = LogicalRegression.sigmoid(thetas.T, Xt)\n",
      "IO.write(res, ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "One vs One"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = IO.read('train.csv')\n",
      "lam = 0.01\n",
      "lr = LogicalRegression(lam)\n",
      "ovo = OneVsOne(lr)\n",
      "thetas = ovo(X, y)\n",
      "Xt, ids = IO.read('test.csv', False)\n",
      "# calculate results on test data\n",
      "ovor = LogicalRegression.sigmoid(thetas, Xt)\n",
      "res = OneVsOne.result(ovor, len(np.unique(y)))\n",
      "IO.write(res, ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Numerical gradient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def numerical_grad(f, params, epsilon):\n",
      "    num_grad = np.zeros(len(params))\n",
      "    perturb = np.zeros(len(params))\n",
      "    for i in range(params.size):\n",
      "        perturb[i] = epsilon\n",
      "        j1 = f(params + perturb)\n",
      "        j2 = f(params - perturb)\n",
      "        num_grad[i] = (j1 - j2) / (2. * epsilon)\n",
      "        perturb[i] = 0\n",
      "    return num_grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Logistical regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#X = np.array([[2,2],[1,3],[2,1],[4,1],[3,3],[2,4]])\n",
      "#y = np.array([0]*3+[1]*3)\n",
      "#X = np.random.rand(1000, 7)\n",
      "#y = (np.random.rand(1000)>0.3).astype(int)\n",
      "#X = np.vstack((np.ones(X.shape[0]), X.T)).T\n",
      "init_theta = np.ones(X.shape[1])\n",
      "\n",
      "\n",
      "ag = LogicalRegression.gradient(init_theta, X, y, 0.01)\n",
      "ng = numerical_grad(lambda params: LogicalRegression.cost(params, X, y, 0.01), init_theta, 1e-4)\n",
      "print(np.sum((ag - ng)**2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1137.8578623\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "array([-0.54635799, -0.05296373, -0.00839387, -0.01550122, -0.01746988,\n",
        "       -0.0143402 , -0.00325129, -0.02333877, -0.14042651, -0.05012274,\n",
        "       -0.02442447, -0.46033208, -0.01974286, -0.03090684, -0.413357  ,\n",
        "       -0.2614573 , -0.09646923, -0.04657415, -0.05060926, -0.01103157,\n",
        "       -0.03465397, -0.02820427, -0.03580463, -0.01670953, -0.37524075,\n",
        "       -0.40116199, -0.06476866, -0.06524016, -0.00970571, -0.03016802,\n",
        "       -0.04020093, -0.01046708, -0.12283252, -0.14822126, -0.74480092,\n",
        "       -0.02204301, -0.12476518, -0.0414027 , -0.02771179, -0.05060768,\n",
        "       -0.32783226, -0.06146541, -0.09818108, -0.05843705, -0.02563301,\n",
        "       -0.01318189, -0.02135658, -0.04962451, -0.29970719, -0.01078902,\n",
        "       -0.0331439 , -0.01439809, -0.03272536, -0.04450138, -0.06103604,\n",
        "       -0.02980543, -0.07010682, -0.05801612, -0.02591078, -0.0469412 ,\n",
        "       -0.1487035 , -0.01900826, -0.11761974, -0.00776374, -0.111683  ,\n",
        "       -0.02189929, -0.04229324, -0.44966373, -0.1109269 , -0.04026648,\n",
        "       -0.06922443, -0.08298725, -0.14194854, -0.04126471, -0.06354287,\n",
        "       -0.23926783, -0.05581486, -0.00480377, -0.03048231, -0.03625142,\n",
        "       -0.0402794 , -0.01166919, -0.00567232, -0.0234572 , -0.01258704,\n",
        "       -0.08729833, -0.27841567, -0.0373847 , -0.19960506, -0.02216539,\n",
        "       -0.03113386, -0.02383752, -0.09078248])"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Softmax regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#X = np.array([[2,2],[1,3],[2,1],[4,1],[3,3],[2,4]])\n",
      "#y = np.array([0]*3+[1]*3)\n",
      "#X = np.random.rand(1000, 7)\n",
      "#y = (np.random.rand(1000)>0.3).astype(int)\n",
      "#X = np.vstack((np.ones(X.shape[0]), X.T)).T\n",
      "#init_theta = np.zeros(X.shape[1])\n",
      "\n",
      "#np.array([[1, 2],[3, 4]]).sum(axis=1)\n",
      "#ag = SoftmaxRegression.gradient(init_theta, X, y, 0.01)\n",
      "#ng = numerical_grad(lambda params: LogicalRegression.cost(params, X, y, 0.01), init_theta, 1e-4)\n",
      "#print(np.sum((ag - ng)**2))\n",
      "\n",
      "init_thetas=np.ones(len(np.unique(y)) * X.shape[1])\n",
      "\n",
      "ag = SoftmaxRegression.gradient(init_thetas, X, y, 0.01, np.unique(y))\n",
      "ng = SoftmaxRegression.cost(init_thetas, X, y, 0.01, np.unique(y))\n",
      "ng = numerical_grad(lambda params: SoftmaxRegression.cost(params, X, y, 0.01, np.unique(y)), init_thetas, 1e-4)\n",
      "print(np.sum((ag - ng)**2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8.57951730616e-12\n"
       ]
      }
     ],
     "prompt_number": 65
    }
   ],
   "metadata": {}
  }
 ]
}