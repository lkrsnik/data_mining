{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "#%matplotlib inline\n",
      "from scipy.stats.stats import pearsonr\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation, preprocessing\n",
      "from collections import defaultdict\n",
      "import Orange"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#descriptors = Orange.data.Table(\"molecular_descriptors_data.txt\")\n",
      "def legend(fn):\n",
      "    with open(fn) as f:\n",
      "        for l in f:\n",
      "            vals_legend = l.strip().split('\\t')[6:]\n",
      "            return vals_legend\n",
      "        \n",
      "\n",
      "# legend\n",
      "LEGEND = legend(\"TrainSet-hw2.txt\")\n",
      "\n",
      "\n",
      "# read train set\n",
      "def fix_dilution(s):\n",
      "    return s.replace('\"', '').strip()\n",
      "\n",
      "def load_data(fn):\n",
      "    readings = defaultdict(list)\n",
      "    with open(fn) as f:\n",
      "        vals_legend = next(f).strip().split('\\t')[6:]\n",
      "        for l in f:\n",
      "            l = l.strip()\n",
      "            t = l.split('\\t')\n",
      "            cid, dilution, vals, intensity = t[0], fix_dilution(t[4]), list(map(float, t[6:])), t[3]\n",
      "            readings[cid, intensity, dilution].append(vals)\n",
      "    for a,b in readings.items():\n",
      "        readings[a] = np.array(b)\n",
      "    return dict(readings)\n",
      "\n",
      "def mean_indv_notnan(data):\n",
      "    means = []\n",
      "    #average non-nan elements\n",
      "    for vals in data.T:\n",
      "        nonnan = vals[~np.isnan(vals)]\n",
      "        means.append(np.mean(nonnan))\n",
      "    return np.array(means)\n",
      "\n",
      "def load_data_mean_indv(fn):\n",
      "    readings = load_data(fn)\n",
      "    r2 = {}\n",
      "    for a,b in readings.items():\n",
      "        r2[a] = mean_indv_notnan(np.array(b))\n",
      "    return r2\n",
      "\n",
      "train_set = load_data_mean_indv(\"TrainSet-hw2.txt\")\n",
      "descriptors = Orange.data.Table(\"molecular_descriptors_data.txt\").X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# returns lines with data\n",
      "def indexes_array(array, value):\n",
      "    b = (array[:,0]==int(value))\n",
      "    indexes = [i for i in range(len(b)) if b[i]]\n",
      "    if indexes == []:\n",
      "        print (\"NOT GOOD\")\n",
      "        return indexes\n",
      "    return array[indexes[0],:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concatenate_array(descriptors, key, value_X, value_Y=[], value=[]):\n",
      "    res = indexes_array(descriptors, key)\n",
      "    if (not(np.isnan(res).any())):\n",
      "        value_X = np.concatenate((value_X, res))\n",
      "        value_Y = np.concatenate((value_Y, value))\n",
      "    return value_X, value_Y\n",
      "\n",
      "def create_train_set(train_set, descriptors):\n",
      "    intensity_X = []\n",
      "    intensity_Y = []\n",
      "    rest_X = []\n",
      "    rest_Y = []\n",
      "    for key, value in train_set.items():\n",
      "        \n",
      "        if key[2] == \"1/1,000\":\n",
      "            intensity_X, intensity_Y = concatenate_array(descriptors, key[0], intensity_X, intensity_Y, [value[0]])\n",
      "        if key[1] == \"high \":\n",
      "            rest_X, rest_Y = concatenate_array(descriptors, key[0], rest_X, rest_Y, value[1:])\n",
      "\n",
      "    intensity_X = intensity_X.reshape((len(intensity_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "    rest_X = rest_X.reshape((len(rest_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "    rest_Y = rest_Y.reshape((len(rest_Y)/20),20)\n",
      "    return intensity_X, intensity_Y, rest_X, rest_Y\n",
      "\n",
      "intensity_X, intensity_Y, rest_X, rest_Y = create_train_set(train_set, descriptors)\n",
      "#print (intensity_Y.shape)\n",
      "#print (intensity_X.shape)\n",
      "#print (intensity_Y.shape)\n",
      "#print (rest_X.shape)\n",
      "#print (rest_Y.shape)\n",
      "#print (rest_Y[:,0].shape)\n",
      "\n",
      "intensity_result = predict(intensity_X, intensity_Y)\n",
      "#print (len(intensity_result))\n",
      "rest_result = []\n",
      "#print (\"HEREEEEEEE\")\n",
      "#print (predict(rest_X, rest_Y[:,0]))\n",
      "#print (rest_X.shape)\n",
      "for i in range(rest_Y.shape[1]):\n",
      "    rest_result = np.concatenate((rest_result, predict(rest_X, rest_Y[:,i])))\n",
      "    break\n",
      "\n",
      "\n",
      "rest_result = rest_result.reshape((len(rest_result)/len(intensity_result)),len(intensity_result))\n",
      "#print (rest_result.shape)\n",
      "#print (rest_result)\n",
      "# prints first line of results\n",
      "#print (rest_result[0,:])\n",
      "#test_set = Orange.data.Table(\"predict.txt\").X[:,0]\n",
      "print (test_set.shape)\n",
      "#print (rest_result.shape)\n",
      "#print (intensity_result.shape)\n",
      "results = np.vstack((intensity_result, rest_result))\n",
      "\n",
      "print (results.shape)\n",
      "#print (results)\n",
      "\n",
      "with open(\"predict.txt\") as f:\n",
      "    with open(\"mean.txt\", \"wt\") as fo:\n",
      "        i = 0\n",
      "        for l in f:\n",
      "            cid, dilution = l.strip().split('\\t')\n",
      "            #print (i)\n",
      "            for j in range(results.shape[0]):\n",
      "                fo.write(\"%s\\t%s\\t%f\\n\" % (cid, LEGEND[j], results[j,i]))\n",
      "            i += 1\n",
      "            #for descriptor, val in zip(LEGEND, results):\n",
      "                \n",
      "\n",
      "#print (intensity_result)\n",
      "#test_characteristics = IO.readData(\"test.tab\")\n",
      "#predictResults(theta, test_characteristics, indexes, 'result')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7894\n",
        "6505\n",
        "9862\n",
        "241\n",
        "660\n",
        "32594\n",
        "9025\n",
        "61331\n",
        "957\n",
        "6920\n",
        "5362588\n",
        "5950\n",
        "11980\n",
        "22873\n",
        "7937\n",
        "6501\n",
        "2969\n",
        "5779\n",
        "16741\n",
        "27457\n",
        "14296\n",
        "62378\n",
        "6569\n",
        "61653\n",
        "8785\n",
        "7921\n",
        "5362814\n",
        "8712\n",
        "11617\n",
        "61408\n",
        "2346\n",
        "7095\n",
        "89440\n",
        "235414\n",
        "7768\n",
        "519539\n",
        "8456\n",
        "7463\n",
        "24116\n",
        "14525\n",
        "326\n",
        "10890\n",
        "637563\n",
        "61185\n",
        "4133\n",
        "106997\n",
        "27458\n",
        "61918\n",
        "12813\n",
        "6057\n",
        "7361\n",
        "228769\n",
        "7749\n",
        "78925\n",
        "15380\n",
        "5610\n",
        "17617\n",
        "7799\n",
        "1110\n",
        "7165\n",
        "7127\n",
        "7761\n",
        "5363388\n",
        "10448\n",
        "6448\n",
        "61027\n",
        "5365027\n",
        "14228\n",
        "7059\n",
        "11614\n",
        "6658\n",
        "1049\n",
        "61641\n",
        "14286\n",
        "62332\n",
        "22310\n",
        "7593\n",
        "311\n",
        "9609\n",
        "444683\n",
        "7894"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "index 79 is out of bounds for axis 1 with size 79",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-e32a30994454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m#print (i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\t%s\\t%f\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEGEND\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#for descriptor, val in zip(LEGEND, results):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index 79 is out of bounds for axis 1 with size 79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6505\n",
        "9862\n",
        "241\n",
        "660\n",
        "32594\n",
        "9025\n",
        "61331\n",
        "957\n",
        "6920\n",
        "5362588\n",
        "5950\n",
        "11980\n",
        "22873\n",
        "7937\n",
        "6501\n",
        "2969\n",
        "5779\n",
        "16741\n",
        "27457\n",
        "14296\n",
        "62378\n",
        "6569\n",
        "61653\n",
        "8785\n",
        "7921\n",
        "5362814\n",
        "8712\n",
        "11617\n",
        "61408\n",
        "2346\n",
        "7095\n",
        "89440\n",
        "235414\n",
        "7768\n",
        "519539\n",
        "8456\n",
        "7463\n",
        "24116\n",
        "14525\n",
        "326\n",
        "10890\n",
        "637563\n",
        "61185\n",
        "4133\n",
        "106997\n",
        "27458\n",
        "61918\n",
        "12813\n",
        "6057\n",
        "7361\n",
        "228769\n",
        "7749\n",
        "78925\n",
        "15380\n",
        "5610\n",
        "17617\n",
        "7799\n",
        "1110\n",
        "7165\n",
        "7127\n",
        "7761\n",
        "5363388\n",
        "10448\n",
        "6448\n",
        "61027\n",
        "5365027\n",
        "14228\n",
        "7059\n",
        "11614\n",
        "6658\n",
        "1049\n",
        "61641\n",
        "14286\n",
        "62332\n",
        "22310\n",
        "7593\n",
        "311\n",
        "9609\n",
        "444683\n",
        "(79,)\n",
        "(2, 79)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_test_set(test_set, descriptors):\n",
      "    test_X = []\n",
      "    i = 0\n",
      "    for key in test_set:\n",
      "        i += 1\n",
      "        #print (key)\n",
      "        test_X, nothing = concatenate_array(descriptors, key, test_X)\n",
      "\n",
      "    test_X = test_X.reshape((len(test_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "    #print (test_X.shape)\n",
      "    return test_X\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CustomFunctions:\n",
      "    #gradient for usage in l_bfsg\n",
      "    def gradient(theta, X, y):\n",
      "        return (1 / y.shape[0]) * ( (X.dot(theta) - y).dot(X))\n",
      "\n",
      "    #cost function for usage in l_bfsg\n",
      "    def costFunction(theta, X, y):\n",
      "        return (1 / (2 * y.shape[0])) * ((X.dot(theta) - y).T.dot(X.dot(theta) - y))\n",
      "\n",
      "    #gradient for usage in l_bfsg\n",
      "    def gradient_regularization(theta, X, y):\n",
      "        return (1 / y.shape[0]) * ( (X.dot(theta) - y).dot(X) + (lam / X.shape[1]) * theta)\n",
      "\n",
      "    #cost function for usage in l_bfsg\n",
      "    def costFunction_regularization(theta, X, y):\n",
      "        return (1 / (2 * y.shape[0])) * ((X.dot(theta) - y).T.dot(X.dot(theta) - y) + lam * theta.T.dot(theta))\n",
      "\n",
      "class IO:\n",
      "    # get learning data\n",
      "    def readData(fileName, setIntensity=False):\n",
      "        if setIntensity:\n",
      "            data = np.loadtxt(fileName, delimiter=\"\\t\", skiprows=3, usecols=(range(0,4870)))\n",
      "        else:\n",
      "            data = np.loadtxt(fileName, delimiter=\"\\t\", skiprows=3, usecols=(range(0,4869)))\n",
      "        characteristics = data[:, 0:4869]\n",
      "        if setIntensity:\n",
      "            intensity = data[:, 4869]\n",
      "            return characteristics, intensity\n",
      "        return characteristics\n",
      "\n",
      "    # testing success on learning data\n",
      "    def printError(X, intensity, theta):\n",
      "        final_learning_data = X.dot(theta)\n",
      "        #print (\"Error on learning data: \" + str(np.sqrt(sum(np.power(final_learning_data - intensity, 2))/len(intensity))))\n",
      "\n",
      "    def exportResults(X, theta, fileName):\n",
      "        file_ = open(fileName, 'w')\n",
      "        for num in X.dot(theta):\n",
      "            file_.write(str(num) + '\\n')\n",
      "        file_.close()\n",
      "        #print X.shape\n",
      "        return X.dot(theta)\n",
      "\n",
      "lam = 0.00003\n",
      "\n",
      "def predict(characteristics, intensity):\n",
      "    #indexes = getBestColumnsPearson(characteristics, intensity)\n",
      "    #theta = learnTheta(characteristics, intensity, indexes, CustomFunctions.costFunction_regularization, CustomFunctions.gradient_regularization)\n",
      "    \n",
      "    \n",
      "    lr = Orange.regression.LassoRegressionLearner(alpha = 0.3) #linearna regresija L2 - lr je objekt, ki se ucije metoda, ki pricakuje podatke\n",
      "    \n",
      "    #print (characteristics.shape)\n",
      "    new_table = Orange.data.Table(characteristics, intensity)\n",
      "    #print (len (new_table))\n",
      "    \n",
      "    model = lr(new_table)\n",
      "    #print (model.skl_model.coef_)\n",
      "    theta = model.skmodel.coef_\n",
      "    #print(theta.shape)\n",
      "    \n",
      "    test_set = []\n",
      "    with open(\"predict.txt\") as f:\n",
      "        i = 0\n",
      "        for l in f:\n",
      "            cid, dilution = l.strip().split('\\t')\n",
      "            test_set = np.concatenate((test_set, [cid]))\n",
      "            i += 1\n",
      "    test_X = create_test_set(test_set, descriptors)\n",
      "    return predictResults(theta, test_X, 'result')\n",
      "\n",
      "#sets argument data and calculates theta using fmin_l_bfsg_b function\n",
      "def learnTheta(characteristics, intensity, indexes, costFun=CustomFunctions.costFunction, gradientFun=CustomFunctions.gradient):\n",
      "    #get arguments for l_bfsg\n",
      "    bestcharact = characteristics[:, indexes]\n",
      "    X = np.column_stack((np.ones(bestcharact.shape[0]), bestcharact))\n",
      "    init_theta = np.zeros(X.shape[1]).T\n",
      "\n",
      "    #calculation of theta (polinome)\n",
      "    theta = fmin_l_bfgs_b(costFun, init_theta, gradientFun, args=(X, intensity))[0]\n",
      "\n",
      "    #calculates if gradient is correct according to cost function\n",
      "    original_gradient = gradientFun(theta, X, intensity)\n",
      "    calculated_gradient = calculate_grad(costFun, X, intensity, theta, 1e-4)\n",
      "\n",
      "\n",
      "    #print testing success on learning data\n",
      "    IO.printError(X, intensity, theta)\n",
      "    return theta\n",
      "\n",
      "def calculate_grad(costFun, X, intensity, theta, eps):\n",
      "    grad = np.zeros(len(theta))\n",
      "    temp = np.zeros(len(theta))\n",
      "    for i in range(0, len(theta)):\n",
      "        temp[i] = eps\n",
      "        j1 = costFun((theta + temp), X, intensity)\n",
      "        j2 = costFun((theta - temp), X, intensity)\n",
      "        grad[i] = (j1 - j2) / (2. * eps)\n",
      "        temp[i] = 0\n",
      "    return grad\n",
      "\n",
      "#calculates results and saves it into file\n",
      "def predictResults(theta, characteristics, fileName):\n",
      "    #bestcharact = characteristics[:, indexes]\n",
      "    #X = np.column_stack((np.ones(bestcharact.shape[0]), bestcharact))\n",
      "    #return IO.exportResults(X, theta, fileName)\n",
      "    return IO.exportResults(characteristics, theta, fileName)\n",
      "\n",
      "#find columns with best corelation with pearson\n",
      "def getBestColumnsPearson(characteristics, intensity):\n",
      "    i = 0\n",
      "    correlations = np.zeros(len(characteristics[0]))\n",
      "    for column in characteristics.T:\n",
      "        correlations[i] = pearsonr(intensity,column)[0]\n",
      "        i += 1\n",
      "    #minint = np.iinfo(np.int32).min\n",
      "    correlations[np.isnan(correlations)] = 0\n",
      "    return np.argpartition(correlations, -19)[-19:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sklearn.__path__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'sklearn' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-39-bbb93fa5c665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__path__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
       ]
      }
     ],
     "prompt_number": 39
    }
   ],
   "metadata": {}
  }
 ]
}