{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "#%matplotlib inline\n",
      "from scipy.stats.stats import pearsonr\n",
      "from scipy.optimize import fmin_l_bfgs_b\n",
      "from sklearn import cross_validation, preprocessing\n",
      "from collections import defaultdict\n",
      "import Orange\n",
      "from scoring import *\n",
      "\n",
      "class DataPrep:\n",
      "    def legend(fn):\n",
      "        with open(fn) as f:\n",
      "            for l in f:\n",
      "                vals_legend = l.strip().split('\\t')[6:]\n",
      "                return vals_legend\n",
      "            \n",
      "    # read train set\n",
      "    def fix_dilution(s):\n",
      "        return s.replace('\"', '').strip()\n",
      "\n",
      "    def load_data(fn):\n",
      "        readings = defaultdict(list)\n",
      "        with open(fn) as f:\n",
      "            vals_legend = next(f).strip().split('\\t')[6:]\n",
      "            for l in f:\n",
      "                l = l.strip()\n",
      "                t = l.split('\\t')\n",
      "                cid, dilution, vals, intensity = t[0], DataPrep.fix_dilution(t[4]), list(map(float, t[6:])), t[3]\n",
      "                readings[cid, intensity, dilution].append(vals)\n",
      "        for a,b in readings.items():\n",
      "            readings[a] = np.array(b)\n",
      "        return dict(readings)\n",
      "\n",
      "    def mean_indv_notnan(data):\n",
      "        means = []\n",
      "        #average non-nan elements\n",
      "        for vals in data.T:\n",
      "            nonnan = vals[~np.isnan(vals)]\n",
      "            means.append(np.mean(nonnan))\n",
      "        return np.array(means)\n",
      "\n",
      "    def load_data_mean_indv(fn):\n",
      "        readings = DataPrep.load_data(fn)\n",
      "        r2 = {}\n",
      "        for a,b in readings.items():\n",
      "            r2[a] = DataPrep.mean_indv_notnan(np.array(b))\n",
      "        return r2\n",
      "\n",
      "class IO:\n",
      "    # get learning data\n",
      "    def readData(fileName):\n",
      "        test_set = []\n",
      "        with open(fileName) as f:\n",
      "            i = 0\n",
      "            for l in f:\n",
      "                cid, dilution = l.strip().split('\\t')\n",
      "                test_set = np.concatenate((test_set, [cid]))\n",
      "                i += 1\n",
      "            return test_set\n",
      "\n",
      "    # testing success on learning data\n",
      "    def printError(X, intensity, theta):\n",
      "        final_learning_data = X.dot(theta)\n",
      "        #print (\"Error on learning data: \" + str(np.sqrt(sum(np.power(final_learning_data - intensity, 2))/len(intensity))))\n",
      "\n",
      "    def exportResults(readFileName, writeFileName, legend, results):\n",
      "        results = results.clip(min=0)\n",
      "        with open(readFileName) as f:\n",
      "            with open(writeFileName, \"wt\") as fo:\n",
      "                i = 0\n",
      "                for l in f:\n",
      "                    cid, dilution = l.strip().split('\\t')\n",
      "                    for j in range(results.shape[0]):\n",
      "                        fo.write(\"%s\\t%s\\t%f\\n\" % (cid, legend[j], results[j,i]))\n",
      "                    i += 1\n",
      "\n",
      "class PipeData:\n",
      "    # returns lines with data\n",
      "    def indexes_array(array, value):\n",
      "        b = (array[:,0]==int(value))\n",
      "        indexes = [i for i in range(len(b)) if b[i]]\n",
      "        if indexes == []:\n",
      "            print (\"NOT GOOD\")\n",
      "            return indexes\n",
      "        return array[indexes[0],:]\n",
      "\n",
      "    def concatenate_array(descriptors, key, value_X, value_Y=[], value=[], train=False):\n",
      "        res = PipeData.indexes_array(descriptors, key)\n",
      "        if (not(np.isnan(res).any())):\n",
      "            value_X = np.concatenate((value_X, res))\n",
      "            value_Y = np.concatenate((value_Y, value))\n",
      "        else:\n",
      "            if (not(train)):\n",
      "                value_X = np.concatenate((value_X, np.zeros(len(res))))\n",
      "        return value_X, value_Y\n",
      "\n",
      "    def create_train_set(train_set, descriptors):\n",
      "        intensity_X = []\n",
      "        intensity_Y = []\n",
      "        rest_X = []\n",
      "        rest_Y = []\n",
      "        for key, value in train_set.items():\n",
      "            \n",
      "            if key[2] == \"1/1,000\":\n",
      "                intensity_X, intensity_Y = PipeData.concatenate_array(descriptors, key[0], intensity_X, intensity_Y, [value[0]], True)\n",
      "            if key[1] == \"high \":\n",
      "                rest_X, rest_Y = PipeData.concatenate_array(descriptors, key[0], rest_X, rest_Y, value[1:], True)\n",
      "\n",
      "        intensity_X = intensity_X.reshape((len(intensity_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "        rest_X = rest_X.reshape((len(rest_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "        rest_Y = rest_Y.reshape((len(rest_Y)/20),20)\n",
      "        return intensity_X, intensity_Y, rest_X, rest_Y\n",
      "\n",
      "    def create_test_set(test_set, descriptors):\n",
      "        test_X = []\n",
      "        i = 0\n",
      "        for key in test_set:\n",
      "            i += 1\n",
      "            test_X, nothing = PipeData.concatenate_array(descriptors, key, test_X)\n",
      "\n",
      "        test_X = test_X.reshape((len(test_X)/descriptors.shape[1]),descriptors.shape[1])\n",
      "        return test_X\n",
      "\n",
      "# calculates a new theta\n",
      "def calculate_model(X, y, alpha):\n",
      "    #lr = Orange.regression.LassoRegressionLearner(alpha = alpha, max_iter = 1000, normalize = False) #linearna regresija L2 - lr je objekt, ki se ucije metoda, ki pricakuje podatke\n",
      "    lr = Orange.regression.linear.LassoRegressionLearner(alpha = alpha) #linearna regresija L2 - lr je objekt, ki se ucije metoda, ki pricakuje podatke\n",
      "    new_table = Orange.data.Table(X, y)\n",
      "    model = lr(new_table)\n",
      "    return model\n",
      "\n",
      "\n",
      "# where the magic happens\n",
      "def magic(descriptors, train_set, test_set, alpha):\n",
      "    # formats train and test set\n",
      "    intensity_X, intensity_Y, rest_X, rest_Y = PipeData.create_train_set(train_set, descriptors)\n",
      "    test_X = PipeData.create_test_set(test_set, descriptors)\n",
      "\n",
      "\n",
      "    print (intensity_X.shape)\n",
      "    print (rest_X.shape)\n",
      "    print (test_X.shape)\n",
      "    \n",
      "\n",
      "    # calculates intensity\n",
      "    intensity_model = calculate_model(intensity_X, intensity_Y, alpha)\n",
      "    intensity_result = intensity_model(test_X)\n",
      "\n",
      "    # calculates others\n",
      "    rest_result = []\n",
      "    for i in range(rest_Y.shape[1]):\n",
      "        rest_model = calculate_model(rest_X, rest_Y[:,i], alpha)\n",
      "        rest_result = np.concatenate((rest_result, rest_model(test_X)))\n",
      "    rest_result = rest_result.reshape((len(rest_result)/len(intensity_result)),len(intensity_result))\n",
      "\n",
      "    # stacks and export results\n",
      "    return np.vstack((intensity_result, rest_result))\n",
      "\n",
      "\n",
      "cross_validation = True\n",
      "alpha = 0.8\n",
      "k = 10\n",
      "alphas = [5, 0.8]\n",
      "\n",
      "# reading data\n",
      "train_set = DataPrep.load_data_mean_indv(\"TrainSet-hw2.txt\")\n",
      "legend = DataPrep.legend(\"TrainSet-hw2.txt\")\n",
      "descriptors = Orange.data.Table(\"molecular_descriptors_data.txt\").X\n",
      "test_set = IO.readData(\"predict.txt\")\n",
      "\n",
      "# izra\u010dunaj rezultat ali izvedi cross validation\n",
      "if (not(cross_validation)):\n",
      "    results = magic(descriptors, train_set, test_set, alpha)\n",
      "    IO.exportResults(\"predict.txt\", \"result\", legend, results)\n",
      "else:\n",
      "    for alpha in alphas:\n",
      "        final = 0\n",
      "        for j in range(k):\n",
      "            i = 0\n",
      "            low_limit = int((len(train_set) * j) / k)\n",
      "            high_limit = int((len(train_set) * (j + 1)) / k)\n",
      "            new_train_set = defaultdict(list)\n",
      "            new_test_set = []\n",
      "            for key, value in train_set.items():\n",
      "                if ((i < high_limit) and (i >= low_limit)):\n",
      "                    new_test_set.append((key[0], key[2]))\n",
      "                else:\n",
      "                    new_train_set[key] = value\n",
      "                i += 1\n",
      "            with open(\"input\", \"wt\") as fo:\n",
      "                for el in new_test_set:\n",
      "                    fo.write(\"%s\\t%s\\n\" % (el[0], el[1]))\n",
      "\n",
      "            # formats test set for magic\n",
      "            magical_test_set = [ a[0] for a in new_test_set ]\n",
      "            results = magic(descriptors, new_train_set, magical_test_set, alpha)\n",
      "            IO.exportResults(\"input\", \"output\", legend, results)\n",
      "            final += score(legend)\n",
      "        print (\"ALPHA =\", alpha)\n",
      "        print (\"THE FINAL\", final / k)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}